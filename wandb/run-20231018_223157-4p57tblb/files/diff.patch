diff --git a/configs/blood_acevedo.yml b/configs/blood_acevedo.yml
index 65d015c..3573058 100644
--- a/configs/blood_acevedo.yml
+++ b/configs/blood_acevedo.yml
@@ -1,2 +1,2 @@
-image_root_path: /path/to/images/folder
+image_root_path: /home/crohling/Documents/cell_classification/images
 class_names: ['BAS','EOS','EBO','MYC','LYT','MON','NGS','PLA']
\ No newline at end of file
diff --git a/configs/blood_matek.yml b/configs/blood_matek.yml
index 8314f5f..65b4c29 100644
--- a/configs/blood_matek.yml
+++ b/configs/blood_matek.yml
@@ -1,2 +1,2 @@
-image_root_path: /path/to/images/folder
+image_root_path: /home/crohling/Documents/cell_classification/images
 class_names: ['BAS','EBO','EOS','KSC','LYA','LYT','MMZ','MOB','MON','MYB','MYO','NGB','NGS','PMB','PMO']
\ No newline at end of file
diff --git a/configs/blood_raabin.yml b/configs/blood_raabin.yml
index 6213ff2..7f232ac 100644
--- a/configs/blood_raabin.yml
+++ b/configs/blood_raabin.yml
@@ -1,2 +1,2 @@
-image_root_path: /path/to/images/folder
+image_root_path: /home/crohling/Documents/cell_classification/images
 class_names : ['BAS','EOS','LYT','MON','NGS']
\ No newline at end of file
diff --git a/configs/bm.yml b/configs/bm.yml
index 1ff9ece..315a31c 100644
--- a/configs/bm.yml
+++ b/configs/bm.yml
@@ -1,3 +1,3 @@
-image_root_path: /path/to/images/folder
+image_root_path: /home/crohling/Documents/cell_classification/images
 class_names: ["ART", "BAS", "BLA", "EBO", "EOS", "FGC", "HAC", "KSC", "LYI", "LYT", "MMZ", "MON", "MYB", "NGB",
                        "NGS", "NIF", "OTH", "PEB", "PLM", "PMO", "ABE"]
\ No newline at end of file
diff --git a/eval_ssl.py b/eval_ssl.py
index 58be433..26c2e76 100644
--- a/eval_ssl.py
+++ b/eval_ssl.py
@@ -28,9 +28,6 @@ from utils.ssl import extract_features, load_data_eval, map_results
 from sparsam.helper import uniform_train_test_splitting, recursive_dict
 
 
-
-
-
 #-----------------------------------------------------------------------------------------------------------------------
 #%%
 def summary_evaluation_results(report_path,n_sample,classifier):
@@ -69,8 +66,9 @@ def summary_evaluation_results(report_path,n_sample,classifier):
     df_results.to_csv(Path(report_path)/'results_mean_std.csv')
 
 
-def evaluate(device,path_to_weights,fit_dataset,eval_dataset,batch_size,model,n_iter,save_dir=None):
-    config_fit, config_eval, train_loader, test_loader =  load_data_eval(fit_dataset,eval_dataset,batch_size=batch_size)
+def evaluate(device,path_to_weights,fit_dataset,eval_dataset,batch_size,model,n_iter,save_dir=None, n_sample_eval=0):
+
+    config_fit, config_eval, train_loader, test_loader =load_data_eval(fit_dataset,eval_dataset,batch_size=batch_size)
     time = datetime.datetime.now()
     if save_dir is None:
         save_dir = Path(config_fit['save_path'])/ Path('Evaluation')/ f'fit_{fit_dataset}_eval_{eval_dataset}' /Path(f'{time.year}_{time.month}_{time.day}-{time.hour}{time.minute}')
@@ -100,10 +98,12 @@ def evaluate(device,path_to_weights,fit_dataset,eval_dataset,batch_size,model,n_
     results_overview = {'n_sample': [], 'iteration': [], 'classifier': [], 'bal_acc': []}
     if n_iter is not None:
         config_fit['n_iterations'] = n_iter
-
+    if n_sample_eval != 0:
+        n_sample_eval = [n_sample_eval]
+        config_fit['eval_class_sizes'] = n_sample_eval
 
     for eval_class_size in tqdm.tqdm(config_fit['eval_class_sizes']):
-        for classifier_name in config_fit['classifier']:
+        for classifier_name in tqdm.tqdm(config_fit['classifier']):
             list_df_metics = []
             cfms = []
             for iteration in range(config_fit['n_iterations']):
@@ -133,19 +133,23 @@ def evaluate(device,path_to_weights,fit_dataset,eval_dataset,batch_size,model,n_
                     dill.dump((features['test'], labels['test'],test_preds), h)
 
                 #save_confusionsmatrix
-                cm = confusion_matrix(eval_labels, test_preds, labels = config_fit['class_names'])
+                class_names = config_fit['class_names']
+
+                eval_label_pred_pair = [(x,y) for x,y in zip(eval_labels, test_preds) if x in class_names]
+                eval_labels, test_preds = list(zip(*eval_label_pred_pair))
+
+                cm = confusion_matrix(eval_labels, test_preds, labels=class_names)
                 cfms.append(np.array(cm))
-                cm_df = pd.DataFrame(cm, index=config_fit['class_names'], columns=config_fit['class_names'])
+                cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)
                 os.makedirs(Path(config_fit['save_path'] )/'cmfs', exist_ok=True)
                 cm_df.to_csv(Path(config_fit['save_path'] )/'cmfs'/ Path(f'confusion_matrix_{classifier_name}_{eval_class_size}_iter_{iteration}.csv'), index=True, header=True)
 
-                precision, recall, f1, _ = precision_recall_fscore_support(eval_labels, test_preds, average=None)
+                precision, recall, f1, _ = precision_recall_fscore_support(eval_labels, test_preds, average=None, labels=class_names)
                 macro_precision = precision_score(eval_labels, test_preds, average='macro')
                 macro_recall = recall_score(eval_labels, test_preds, average='macro')
                 macro_f1 = f1_score(eval_labels, test_preds, average='macro')
-                class_labels = np.unique(eval_labels)
                 metrics_df = pd.DataFrame({'Precision': precision, 'Recall': recall, 'F1 Score': f1},
-                                          index=class_labels)
+                                          index=class_names)
                 metrics_df.loc['All'] = [macro_precision, macro_recall, macro_f1]
 
                 os.makedirs(Path(config_fit['save_path']) / 'metrics', exist_ok=True)
@@ -233,7 +237,7 @@ def evaluate(device,path_to_weights,fit_dataset,eval_dataset,batch_size,model,n_
     help='To only use #number of images to train self-supervised'
 )
 @click.option(
-    '--n_iter', type=click.INT,required=False, default = 1,
+    '--n_iter', type=click.INT, required=False, default=1,
     help='How often the classifier is fitted using random images per class'
 )
 @click.option(
@@ -244,9 +248,8 @@ def evaluate(device,path_to_weights,fit_dataset,eval_dataset,batch_size,model,n_
     '--classifier', type=click.STRING, required=False, default='ALL',
     help='choose SVC, LR, KNN or ALL'
  )
-
-def run(device,train,path_to_weights,dataset_fit,dataset_eval,batch_size,teacher_momentum,model,save_path,max_train_image_number,n_iter,
-        n_sample_eval,classifier,):
+def run(device, train, path_to_weights, dataset_fit, dataset_eval, batch_size, teacher_momentum, model, save_path,max_train_image_number,n_iter,
+        n_sample_eval, classifier,):
 
 
     config, data_set_config = load_config(dataset_fit)
@@ -276,7 +279,7 @@ def run(device,train,path_to_weights,dataset_fit,dataset_eval,batch_size,teacher
     data_loader_parameter['drop_last'] = False
 
 
-    report_path = evaluate(device, path_to_weights, dataset_fit,dataset_eval, batch_size, model, n_iter=n_iter,save_dir=Path(config['save_path'])/'evaluation')
+    report_path = evaluate(device, path_to_weights, dataset_fit,dataset_eval, batch_size, model, n_iter=n_iter,save_dir=Path(config['save_path'])/'evaluation', n_sample_eval=n_sample_eval)
     #update save parameter
     save_parameter = {'dataset': f'fit_{dataset_fit}_eval_{dataset_eval}', 'device': device, 'if train': train, 'path_to_weights': path_to_weights,
                       'batch_size': batch_size,
@@ -289,10 +292,10 @@ def run(device,train,path_to_weights,dataset_fit,dataset_eval,batch_size,teacher
         config['n_iterations'] = n_sample_eval
 
 
-
     summary_evaluation_results(report_path, n_sample=config['n_iterations'], classifier=classifier)
 
 if __name__ == "__main__":
     warnings.filterwarnings("ignore", category=UndefinedMetricWarning, module='sklearn.metrics')
     warnings.filterwarnings("ignore", category=UserWarning)
     run()
+
diff --git a/wandb/latest-run b/wandb/latest-run
index 213af21..78fed0e 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20230919_171702-e7y9t7mn
\ No newline at end of file
+run-20231018_223157-4p57tblb
\ No newline at end of file
